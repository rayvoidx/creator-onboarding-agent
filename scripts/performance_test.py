"""LangGraph ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸"""

import asyncio
import time
import logging
import statistics
from typing import Dict, Any, List
import json
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.graphs.main_orchestrator import get_orchestrator
from src.rag.rag_pipeline import RAGPipeline
from src.rag.prompt_templates import PromptType
from src.monitoring.performance_monitor import PerformanceMonitor
from src.monitoring.metrics_collector import MetricsCollector

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PerformanceTester:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.orchestrator = None
        self.rag_pipeline = None
        self.performance_monitor = PerformanceMonitor()
        self.metrics_collector = MetricsCollector()
        self.test_results = []
    
    async def initialize_systems(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        try:
            # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
            self.orchestrator = get_orchestrator({
                'database_url': 'sqlite:///test.db',
                'redis_url': 'redis://localhost:6379/0',
                'vector_db_config': {'chroma_path': './test_chroma_db'},
                'llm_configs': {
                    'openai_api_key': os.getenv('OPENAI_API_KEY', ''),
                    'anthropic_api_key': os.getenv('ANTHROPIC_API_KEY', ''),
                    'default_model': 'gpt-5.1',
                    'fallback_model': 'claude-3-sonnet'
                }
            })
            
            # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
            self.rag_pipeline = RAGPipeline({
                'retrieval': {
                    'vector_weight': 0.7,
                    'keyword_weight': 0.3,
                    'max_results': 10
                },
                'generation': {
                    'default_model': 'gpt-5.1',
                    'fallback_model': 'claude-3-sonnet',
                    'openai_api_key': os.getenv('OPENAI_API_KEY', ''),
                    'anthropic_api_key': os.getenv('ANTHROPIC_API_KEY', '')
                }
            })
            
            logger.info("Systems initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize systems: {e}")
            raise
    
    async def test_langgraph_latency(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """LangGraph ì§€ì—°ì‹œê°„ í…ŒìŠ¤íŠ¸"""
        logger.info("Starting LangGraph latency tests...")
        
        results = {
            'test_name': 'LangGraph Latency Test',
            'total_tests': len(test_cases),
            'successful_tests': 0,
            'failed_tests': 0,
            'latencies': [],
            'errors': []
        }
        
        for i, test_case in enumerate(test_cases):
            try:
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
                operation_id = await self.performance_monitor.start_operation(
                    f"langgraph_test_{i}",
                    {'test_case': test_case}
                )
                
                start_time = time.time()
                
                # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì‹¤í–‰
                result = await self.orchestrator.run({
                    'message': test_case['message'],
                    'user_id': test_case.get('user_id', 'test_user'),
                    'session_id': test_case.get('session_id', f'test_session_{i}'),
                    'context': test_case.get('context', {})
                })
                
                end_time = time.time()
                latency = end_time - start_time
                
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
                await self.performance_monitor.end_operation(
                    operation_id,
                    success=result.get('success', False),
                    metadata={'latency': latency}
                )
                
                results['latencies'].append(latency)
                
                if result.get('success', False):
                    results['successful_tests'] += 1
                    logger.info(f"Test {i+1} completed successfully in {latency:.3f}s")
                else:
                    results['failed_tests'] += 1
                    results['errors'].append({
                        'test_case': i,
                        'error': result.get('error', 'Unknown error')
                    })
                    logger.warning(f"Test {i+1} failed: {result.get('error', 'Unknown error')}")
                
            except Exception as e:
                results['failed_tests'] += 1
                results['errors'].append({
                    'test_case': i,
                    'error': str(e)
                })
                logger.error(f"Test {i+1} failed with exception: {e}")
        
        # í†µê³„ ê³„ì‚°
        if results['latencies']:
            results['avg_latency'] = statistics.mean(results['latencies'])
            results['min_latency'] = min(results['latencies'])
            results['max_latency'] = max(results['latencies'])
            results['p95_latency'] = self._calculate_percentile(results['latencies'], 95)
            results['p99_latency'] = self._calculate_percentile(results['latencies'], 99)
        
        results['success_rate'] = results['successful_tests'] / results['total_tests'] if results['total_tests'] > 0 else 0
        
        return results
    
    async def test_rag_accuracy(self, test_cases: List[Dict[str, Any]]) -> Dict[str, Any]:
        """RAG ì •í™•ë„ í…ŒìŠ¤íŠ¸"""
        logger.info("Starting RAG accuracy tests...")
        
        results = {
            'test_name': 'RAG Accuracy Test',
            'total_tests': len(test_cases),
            'successful_tests': 0,
            'failed_tests': 0,
            'processing_times': [],
            'retrieval_scores': [],
            'generation_quality': [],
            'errors': []
        }
        
        for i, test_case in enumerate(test_cases):
            try:
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
                operation_id = await self.performance_monitor.start_operation(
                    f"rag_test_{i}",
                    {'test_case': test_case}
                )
                
                start_time = time.time()
                
                # RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                result = await self.rag_pipeline.process_query(
                    query=test_case['query'],
                    query_type=test_case.get('query_type', PromptType.GENERAL_CHAT),
                    user_context=test_case.get('user_context', {}),
                    conversation_history=test_case.get('conversation_history', [])
                )
                
                end_time = time.time()
                processing_time = end_time - start_time
                
                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
                await self.performance_monitor.end_operation(
                    operation_id,
                    success=result.get('success', False),
                    metadata={
                        'processing_time': processing_time,
                        'num_documents': len(result.get('retrieved_documents', []))
                    }
                )
                
                results['processing_times'].append(processing_time)
                
                if result.get('success', False):
                    results['successful_tests'] += 1
                    
                    # ê²€ìƒ‰ ì ìˆ˜ ê³„ì‚°
                    retrieved_docs = result.get('retrieved_documents', [])
                    if retrieved_docs:
                        avg_score = sum(doc.get('score', 0) for doc in retrieved_docs) / len(retrieved_docs)
                        results['retrieval_scores'].append(avg_score)
                    
                    # ìƒì„± í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                    response = result.get('response', '')
                    quality_score = self._evaluate_response_quality(response, test_case.get('expected_keywords', []))
                    results['generation_quality'].append(quality_score)
                    
                    logger.info(f"RAG test {i+1} completed successfully in {processing_time:.3f}s")
                else:
                    results['failed_tests'] += 1
                    results['errors'].append({
                        'test_case': i,
                        'error': result.get('error', 'Unknown error')
                    })
                    logger.warning(f"RAG test {i+1} failed: {result.get('error', 'Unknown error')}")
                
            except Exception as e:
                results['failed_tests'] += 1
                results['errors'].append({
                    'test_case': i,
                    'error': str(e)
                })
                logger.error(f"RAG test {i+1} failed with exception: {e}")
        
        # í†µê³„ ê³„ì‚°
        if results['processing_times']:
            results['avg_processing_time'] = statistics.mean(results['processing_times'])
            results['min_processing_time'] = min(results['processing_times'])
            results['max_processing_time'] = max(results['processing_times'])
        
        if results['retrieval_scores']:
            results['avg_retrieval_score'] = statistics.mean(results['retrieval_scores'])
        
        if results['generation_quality']:
            results['avg_generation_quality'] = statistics.mean(results['generation_quality'])
        
        results['success_rate'] = results['successful_tests'] / results['total_tests'] if results['total_tests'] > 0 else 0
        
        return results
    
    def _calculate_percentile(self, data: List[float], percentile: int) -> float:
        """ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°"""
        if not data:
            return 0.0
        
        sorted_data = sorted(data)
        index = int((percentile / 100) * len(sorted_data))
        return sorted_data[min(index, len(sorted_data) - 1)]
    
    def _evaluate_response_quality(self, response: str, expected_keywords: List[str]) -> float:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        if not response or not expected_keywords:
            return 0.5
        
        response_lower = response.lower()
        matched_keywords = sum(1 for keyword in expected_keywords if keyword.lower() in response_lower)
        
        return matched_keywords / len(expected_keywords)
    
    async def run_comprehensive_test(self):
        """ì¢…í•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("Starting comprehensive performance test...")
        
        # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì •ì˜
        langgraph_test_cases = [
            {
                'message': 'ìœ¡ì•„ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”',
                'user_id': 'test_user_1',
                'session_id': 'test_session_1',
                'context': {'interests': ['ìœ¡ì•„', 'êµìœ¡']}
            },
            {
                'message': 'ì•„ì´ ë°œë‹¬ ë‹¨ê³„ë³„ íŠ¹ì§•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”',
                'user_id': 'test_user_2',
                'session_id': 'test_session_2',
                'context': {'interests': ['ë°œë‹¬', 'ìœ¡ì•„']}
            },
            {
                'message': 'ë¶€ëª¨êµìœ¡ í”„ë¡œê·¸ë¨ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”',
                'user_id': 'test_user_3',
                'session_id': 'test_session_3',
                'context': {'interests': ['êµìœ¡', 'í”„ë¡œê·¸ë¨']}
            }
        ]
        
        rag_test_cases = [
            {
                'query': 'ìœ¡ì•„ ì •ì±…ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”',
                'query_type': PromptType.GENERAL_CHAT,
                'user_context': {'interests': ['ì •ì±…', 'ìœ¡ì•„']},
                'expected_keywords': ['ì •ì±…', 'ìœ¡ì•„', 'ì§€ì›']
            },
            {
                'query': 'ì•„ë™ ë°œë‹¬ ì´ë¡ ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”',
                'query_type': PromptType.SEARCH,
                'user_context': {'interests': ['ë°œë‹¬', 'ì´ë¡ ']},
                'expected_keywords': ['ë°œë‹¬', 'ì´ë¡ ', 'ë‹¨ê³„']
            }
        ]
        
        try:
            # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
            await self.initialize_systems()
            
            # LangGraph ì§€ì—°ì‹œê°„ í…ŒìŠ¤íŠ¸
            langgraph_results = await self.test_langgraph_latency(langgraph_test_cases)
            self.test_results.append(langgraph_results)
            
            # RAG ì •í™•ë„ í…ŒìŠ¤íŠ¸
            rag_results = await self.test_rag_accuracy(rag_test_cases)
            self.test_results.append(rag_results)
            
            # ì„±ëŠ¥ ìš”ì•½ ìƒì„±
            performance_summary = self.performance_monitor.get_performance_summary()
            
            # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            system_metrics = await self.metrics_collector.collect_system_metrics()
            
            # ìµœì¢… ê²°ê³¼ ìƒì„±
            final_results = {
                'test_timestamp': datetime.now().isoformat(),
                'test_results': self.test_results,
                'performance_summary': performance_summary,
                'system_metrics': system_metrics,
                'overall_assessment': self._generate_overall_assessment()
            }
            
            # ê²°ê³¼ ì €ì¥
            with open('performance_test_results.json', 'w', encoding='utf-8') as f:
                json.dump(final_results, f, ensure_ascii=False, indent=2)
            
            # ê²°ê³¼ ì¶œë ¥
            self._print_test_results(final_results)
            
            return final_results
            
        except Exception as e:
            logger.error(f"Comprehensive test failed: {e}")
            raise
    
    def _generate_overall_assessment(self) -> Dict[str, Any]:
        """ì „ì²´ í‰ê°€ ìƒì„±"""
        assessment = {
            'langgraph_performance': 'Good',
            'rag_accuracy': 'Good',
            'system_stability': 'Good',
            'recommendations': []
        }
        
        # LangGraph ì„±ëŠ¥ í‰ê°€
        langgraph_result = next((r for r in self.test_results if r['test_name'] == 'LangGraph Latency Test'), None)
        if langgraph_result:
            avg_latency = langgraph_result.get('avg_latency', 0)
            success_rate = langgraph_result.get('success_rate', 0)
            
            if avg_latency > 10:
                assessment['langgraph_performance'] = 'Poor'
                assessment['recommendations'].append('LangGraph ì§€ì—°ì‹œê°„ì´ ë†’ìŠµë‹ˆë‹¤. ì›Œí¬í”Œë¡œìš° ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.')
            elif avg_latency > 5:
                assessment['langgraph_performance'] = 'Fair'
                assessment['recommendations'].append('LangGraph ì§€ì—°ì‹œê°„ì„ ê°œì„ í•  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.')
            
            if success_rate < 0.8:
                assessment['system_stability'] = 'Poor'
                assessment['recommendations'].append('ì‹œìŠ¤í…œ ì•ˆì •ì„±ì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.')
        
        # RAG ì •í™•ë„ í‰ê°€
        rag_result = next((r for r in self.test_results if r['test_name'] == 'RAG Accuracy Test'), None)
        if rag_result:
            avg_quality = rag_result.get('avg_generation_quality', 0)
            success_rate = rag_result.get('success_rate', 0)
            
            if avg_quality < 0.5:
                assessment['rag_accuracy'] = 'Poor'
                assessment['recommendations'].append('RAG ì‘ë‹µ í’ˆì§ˆì„ ê°œì„ í•´ì•¼ í•©ë‹ˆë‹¤.')
            elif avg_quality < 0.7:
                assessment['rag_accuracy'] = 'Fair'
                assessment['recommendations'].append('RAG ì‘ë‹µ í’ˆì§ˆì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.')
        
        return assessment
    
    def _print_test_results(self, results: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("LangGraph AI Learning System - Performance Test Results")
        print("="*80)
        
        for test_result in results['test_results']:
            print(f"\nğŸ“Š {test_result['test_name']}")
            print("-" * 50)
            print(f"Total Tests: {test_result['total_tests']}")
            print(f"Successful: {test_result['successful_tests']}")
            print(f"Failed: {test_result['failed_tests']}")
            print(f"Success Rate: {test_result.get('success_rate', 0):.2%}")
            
            if 'avg_latency' in test_result:
                print(f"Average Latency: {test_result['avg_latency']:.3f}s")
                print(f"Min Latency: {test_result['min_latency']:.3f}s")
                print(f"Max Latency: {test_result['max_latency']:.3f}s")
                print(f"P95 Latency: {test_result['p95_latency']:.3f}s")
            
            if 'avg_processing_time' in test_result:
                print(f"Average Processing Time: {test_result['avg_processing_time']:.3f}s")
            
            if 'avg_generation_quality' in test_result:
                print(f"Average Generation Quality: {test_result['avg_generation_quality']:.2f}")
        
        print("ğŸ¯ Overall Assessment")
        print("-" * 50)
        assessment = results['overall_assessment']
        print(f"LangGraph Performance: {assessment['langgraph_performance']}")
        print(f"RAG Accuracy: {assessment['rag_accuracy']}")
        print(f"System Stability: {assessment['system_stability']}")
        
        if assessment['recommendations']:
            print("ğŸ’¡ Recommendations:")
            for i, rec in enumerate(assessment['recommendations'], 1):
                print(f"  {i}. {rec}")
        
        print("ğŸ“ Detailed results saved to: performance_test_results.json")
        print("="*80)


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = PerformanceTester()
    
    try:
        results = await tester.run_comprehensive_test()
        logger.info("Performance test completed successfully")
        return results
        
    except Exception as e:
        logger.error(f"Performance test failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
